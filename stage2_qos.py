# -*- coding: utf-8 -*-
"""Stage2_QoS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhKeQMxW8mLAzk4tlvwnsZZNSF_y070z

# **Prerequisite**
"""

pip install keras

pip install keras-tuner

!pip install lightgbm

import joblib
import kagglehub
import keras_tuner as kt
import lightgbm as lgb
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import random
import tensorflow as tf
import timeit
import zipfile
from collections import deque
from kagglehub import KaggleDatasetAdapter
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve,f1_score
from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,cross_val_score,StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler,MultiLabelBinarizer,label_binarize
from tensorflow.keras.utils import to_categorical
from xgboost import XGBClassifier

"""# **Data Collection**"""

# Load Dataset from Public Resource
!kaggle datasets download -d jsrojas/ip-network-traffic-flows-labeled-with-87-apps

with zipfile.ZipFile('ip-network-traffic-flows-labeled-with-87-apps.zip', 'r') as zip_ref:
    zip_ref.extractall('.')
data = pd.read_csv('Dataset-Unicauca-Version2-87Atts.csv')

data['ProtocolName'].unique()
data['ProtocolName'].value_counts()

# Map ProtocolName to Application Category according to https://www.ntop.org/products/deep-packet-inspection/ndpi/
# "Others" means not found in ntop supported protocols
# "Video" is included in "Streaming"
mapping = {
    '99TAXI':'Others',
    'AMAZON':'Web',
    'APPLE':'Web',
    'APPLE_ICLOUD':'Web',
    'APPLE_ITUNES':'Streaming',
    'BGP':'Network',
    'BITTORRENT':'Download',
    'CITRIX':'Network',
    'CITRIX_ONLINE':'Network',
    'CLOUDFLARE':'Web',
    'CNN':'Web',
    'CONTENT_FLASH':'Streaming',
    'DEEZER':'Music',
    'DNS':'Network',
    'DROPBOX':'Cloud',
    'EASYTAXI':'Others',
    'EBAY':'Shopping',
    'EDONKEY':'Download',
    'FACEBOOK':'SocialMedia',
    'FTP_CONTROL':'Download',
    'FTP_DATA':'Download',
    'GMAIL':'Email',
    'GOOGLE':'Web',
    'GOOGLE_MAPS':'Web',
    'H323':'VoIP',
    'HTTP':'Web',
    'HTTP_CONNECT':'Web',
    'HTTP_DOWNLOAD':'Download',
    'HTTP_PROXY':'Web',
    'INSTAGRAM':'SocialMedia',
    'IP_ICMP':'Network',
    'IP_OSPF':'Network',
    'LASTFM':'Music',
    'LOTUS_NOTES':'Collaborative',
    'MAIL_IMAPS':'Email',
    'MICROSOFT':'Cloud',
    'MQTT':'RPC',
    'MS_ONE_DRIVE':'Cloud',
    'MSN':'Others',
    'MSSQL':'Database',
    'NETFLIX':'Streaming',
    'NFS':'DataTransfer',
    'NTP':'System',
    'OFFICE_365':'Collaborative',
    'OPENSIGNAL':'Others',
    'OPENVPN':'VPN',
    'ORACLE':'Database',
    'OSCAR':'Others',
    'QQ':'Chat',
    'RADIUS':'Network',
    'RTMP':'Streaming',
    'SIMET':'Others',
    'SKINNY':'VoIP',
    'SKYPE':'VoIP',
    'SNMP':'Network',
    'SOCKS':'Web',
    'SPOTIFY':'Music',
    'SSH':'RemoteAccess',
    'SSL':'Others',
    'SSL_NO_CERT':'Others',
    'STARCRAFT':'Game',
    'TEAMSPEAK':'VoIP',
    'TEAMVIEWER':'RemoteAccess',
    'TELEGRAM':'Chat',
    'TIMMEU':'Others',
    'TOR':'VPN',
    'TWITCH':'Streaming',
    'TWITTER':'SocialMedia',
    'UBUNTUONE':'Cloud',
    'UNENCRYPED_JABBER':'Others',
    'UPNP':'Others',
    'WAZE':'Web',
    'WHATSAPP':'Chat',
    'WHOIS_DAS':'Network',
    'WIKIPEDIA':'Web',
    'WINDOWS_UPDATE':'SoftwareUpdate',
    'YAHOO':'Web',
    'YOUTUBE':'Streaming',
    }
data['AppCat'] = data['ProtocolName'].map(mapping)
data['AppCat'].value_counts()

# Keep only the top 9 AppCat_encoded entries
top_9_appcats = data['AppCat'].value_counts().nlargest(9).index
data_top_9 = data[data['AppCat'].isin(top_9_appcats)]

data_top_9['AppCat'].unique()
data_top_9['AppCat'].value_counts()

data_top_9.shape[0]

df = data_top_9

"""# **Data Preprocessing**"""

# Encode Application Category
label_mapping = {'Web': 1, 'Streaming': 2, 'Cloud': 3, 'SocialMedia': 4, 'Email': 5, 'SoftwareUpdate': 6, 'VoIP':7, 'Collaborative':8, 'Others': 0}
df['AppCat_encoded'] = df['AppCat'].map(label_mapping)

# Drop non-numerical column
df.drop(['Flow.ID','Source.IP','Destination.IP','Timestamp','Label','L7Protocol'],inplace=True, axis=1)
df.info()

# Separate features (x) and target variable (y)
x = df.drop(['AppCat', 'AppCat_encoded', 'ProtocolName'], axis=1)
y = df['AppCat_encoded']

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Replace inf and large values with NaN
x_train = x_train.replace([np.inf, -np.inf], np.nan)
# Impute NaN values (e.g., with the mean)
x_train = x_train.fillna(x_train.mean())

# Fitting XGB model for Feature Selection
xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)
xgb_model.fit(x_train, y_train)

# Get feature importances
feature_importances = xgb_model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': x_train.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df

# Select Top 50 Features
top_n = 50
selected_features = feature_importance_df.iloc[:top_n]['Feature'].tolist()

# Use only selected features for further training
x_train_selected = x_train[selected_features]
x_test_selected = x_test[selected_features]

# Replace inf and large values with NaN
x_train_selected = x_train_selected.replace([np.inf, -np.inf], np.nan)
x_test_selected = x_test_selected.replace([np.inf, -np.inf], np.nan)
# Impute NaN values (e.g., with the mean)
x_train_selected = x_train_selected.fillna(x_train.mean())
x_test_selected = x_test_selected.fillna(x_train.mean())

pip install imblearn

from imblearn.over_sampling import SMOTE

# Handle Class Imbalance
smote = SMOTE()
x_train_resampled, y_train_resampled = smote.fit_resample(x_train_selected, y_train)

# Scaling data
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train_resampled)
x_test_scaled = scaler.transform(x_test_selected)

"""# **Model Training: Random Forest**"""

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 15, 20, 30, None],
    'max_features': ['sqrt', 'log2', 0.3, 0.5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 5],
    'bootstrap': [True, False]
}

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')

# Perform Randomized Search with Cross-Validation
rf_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

# Train model with best hyperparameters
rf_search.fit(x_train_scaled, y_train_resampled)

# Best parameters and score
print("Best Parameters:", rf_search.best_params_)
print("Best Score:", rf_search.best_score_)

# Train Best Model
best_rf = rf_search.best_estimator_
best_rf.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_rf, 'best_rf_model.pkl')

# Predictions
y_pred_RF = best_rf.predict(x_test_scaled)
y_prob_RF = best_rf.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_rf = accuracy_score(y_test, y_pred_RF)
print(f"Accuracy: {accuracy_rf:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_RF))
print("Classification Report:\n", classification_report(y_test, y_pred_RF))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_rf = cross_val_score(best_rf, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"RF Validation Score: {np.mean(cv_scores_rf):.4f} ± {np.std(cv_scores_rf):.4f}")

# ROC AUC score
roc_auc_rf = roc_auc_score(y_test, y_prob_RF, multi_class='ovr')
print(f"RF ROC AUC Score: {roc_auc_rf:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_rf = dict()
tpr_rf = dict()
roc_auc_per_class_rf = dict()

for i in range(n_classes):
    fpr_rf[i], tpr_rf[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_RF[:, i])
    roc_auc_per_class_rf[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_RF[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_rf[i], tpr_rf[i], label=f'Class {i} (AUC = {roc_auc_per_class_rf[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: LightGBM**"""

# Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 500],  # Number of trees
    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate
    'max_depth': [-1, 5, 10],  # Tree depth (-1 means no limit)
    'num_leaves': [20, 31, 40],  # Number of leaves
    'min_child_samples': [10, 20, 50],  # Minimum samples per leaf
    'subsample': [0.7, 0.8, 1.0],  # Fraction of data used per tree
    'colsample_bytree': [0.7, 0.8, 1.0],  # Feature fraction
    'reg_alpha': [0, 0.01, 0.1],  # L1 regularization
    'reg_lambda': [0, 0.01, 0.1]  # L2 regularization
}

# LightGBM Model
lgb_model = lgb.LGBMClassifier(objective='multiclass', random_state=42)

# Set up RandomizedSearchCV
lgb_search = RandomizedSearchCV(
    estimator=lgb_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True,
)

lgb_search.fit(x_train_scaled, y_train_resampled)

# Best hyperparameters and score
print("Best Parameters:", lgb_search.best_params_)
print("Best Score:", lgb_search.best_score_)

# Train Best Model
best_lgb = lgb_search.best_estimator_
best_lgb.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_lgb, 'best_lgb_model.pkl')

# Predictions
y_pred_LGB = best_lgb.predict(x_test_scaled)
y_prob_LGB = best_lgb.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_lgb = accuracy_score(y_test, y_pred_LGB)
print(f"Accuracy: {accuracy_lgb:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_LGB))
print("Classification Report:\n", classification_report(y_test, y_pred_LGB))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_lgb = cross_val_score(best_lgb, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"LGB Validation Score: {np.mean(cv_scores_lgb):.4f} ± {np.std(cv_scores_lgb):.4f}")

# ROC AUC score
roc_auc_lgb = roc_auc_score(y_test, y_prob_LGB, multi_class='ovr')
print(f"LGB ROC AUC Score: {roc_auc_lgb:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_lgb = dict()
tpr_lgb = dict()
roc_auc_per_class_lgb = dict()

for i in range(n_classes):
    fpr_lgb[i], tpr_lgb[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_LGB[:, i])
    roc_auc_per_class_lgb[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_LGB[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_lgb[i], tpr_lgb[i], label=f'Class {i} (AUC = {roc_auc_per_class_lgb[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: XGBoost**"""

# Hyperparameter Tuning
param_grid = {
    'estimator__n_estimators': [100, 200, 300, 500],
    'estimator__max_depth': [3, 5, 6, 8],
    'estimator__learning_rate': [0.001, 0.01, 0.05, 0.1],
    'estimator__subsample': [0.7, 0.8, 1.0],
    'estimator__colsample_bytree': [0.7, 0.8, 1.0],
    'estimator__gamma': [0, 0.1, 0.3, 0.5],
    'estimator__min_child_weight': [1, 3, 5],
    'estimator__reg_alpha': [0, 0.01, 0.1],
    'estimator__reg_lambda': [0, 0.01, 0.1]
}

# XGBoost Base Model
xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=9,  # Number of classes for AppCat classification
    eval_metric='mlogloss',
    use_label_encoder=False,
    random_state=42
)

# RandomizedSearchCV with 5-Fold Cross Validation
xgb_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

# Fit Model
xgb_search.fit(x_train_scaled, y_train_resampled)

# Best Parameters and Score
print("Best Parameters (AppCat):", xgb_search.best_params_)
print("Best Score (AppCat):", xgb_search.best_score_)

# Train Best Model
best_xgb = xgb_search.best_estimator_
best_xgb.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_xgb, 'best_xgb_model.pkl')

# Predictions
y_pred_XGB = best_xgb.predict(x_test_scaled)
y_prob_XGB = best_xgb.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_xgb = accuracy_score(y_test, y_pred_XGB)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_XGB))
print("Classification Report:\n", classification_report(y_test, y_pred_XGB))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_xgb = cross_val_score(best_xgb, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"XGB Validation Score: {np.mean(cv_scores_xgb):.4f} ± {np.std(cv_scores_xgb):.4f}")

# ROC AUC score
roc_auc_xgb = roc_auc_score(y_test, y_prob_XGB, multi_class='ovr')
print(f"RF ROC AUC Score: {roc_auc_xgb:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_xgb = dict()
tpr_xgb = dict()
roc_auc_per_class_xgb = dict()

for i in range(n_classes):
    fpr_xgb[i], tpr_xgb[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_XGB[:, i])
    roc_auc_per_class_rf[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_XGB[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_xgb[i], tpr_xgb[i], label=f'Class {i} (AUC = {roc_auc_per_class_xgb[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: K-Nearest Neighbors**"""

# Define hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 10],  # Number of neighbors
    'weights': ['uniform', 'distance'],  # Weight function
    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metric
}

# Base KNN model (no random_state, as KNN is non-deterministic)
knn_model = KNeighborsClassifier()

# Perform Randomized Search with Cross-Validation
knn_search = RandomizedSearchCV(
    estimator=knn_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

# Fit the model
knn_search.fit(x_train_scaled, y_train_resampled)

# Best hyperparameters and score
print("Best Parameters:", knn_search.best_params_)
print("Best Score:", knn_search.best_score_)

# Train Best Model
best_knn = knn_search.best_estimator_
best_knn.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_knn, 'best_knn_model.pkl')

# Predictions
y_pred_KNN = best_knn.predict(x_test_scaled)
y_prob_KNN = best_knn.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_knn = accuracy_score(y_test, y_pred_KNN)
print(f"Accuracy: {accuracy_knn:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_KNN))
print("Classification Report:\n", classification_report(y_test, y_pred_KNN))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_knn = cross_val_score(best_knn, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"KNN Validation Score: {np.mean(cv_scores_knn):.4f} ± {np.std(cv_scores_knn):.4f}")

# ROC AUC score
roc_auc_knn = roc_auc_score(y_test, y_prob_KNN, multi_class='ovr')
print(f"KNN ROC AUC Score: {roc_auc_knn:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_knn = dict()
tpr_knn = dict()
roc_auc_per_class_knn = dict()

for i in range(n_classes):
    fpr_knn[i], tpr_knn[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_KNN[:, i])
    roc_auc_per_class_knn[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_KNN[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_knn[i], tpr_knn[i], label=f'Class {i} (AUC = {roc_auc_per_class_knn[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()
# -*- coding: utf-8 -*-
"""Stage1_Security.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9GWIUGXZht1i6Wi5zmvLJm05P4WBrXY

# **Prerequisite**
"""

pip install keras

pip install keras-tuner

import joblib
import kagglehub
import keras_tuner as kt
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import random
import tensorflow as tf
import timeit
import zipfile
from collections import deque
from kagglehub import KaggleDatasetAdapter
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve
from sklearn.model_selection import train_test_split,RandomizedSearchCV,cross_val_score,StratifiedKFold
from sklearn.svm import SVC
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,LeakyReLU,Activation,LSTM,Bidirectional,GRU
from tensorflow.keras.optimizers import Adam
from xgboost import XGBClassifier

"""# **Data Collection**"""

# Load Dataset from Public Resource
!kaggle datasets download -d solarmainframe/ids-intrusion-csv

folder_name = "csv_files"
if not os.path.exists(folder_name):
    os.makedirs(folder_name)

with zipfile.ZipFile('ids-intrusion-csv.zip', 'r') as zip_ref:
    for file_info in zip_ref.infolist():
        if file_info.filename.endswith('.csv'):
            file_info.filename = os.path.join(folder_name, os.path.basename(file_info.filename))
            zip_ref.extract(file_info)

# Merge all the unzipped csv file into a single dataset
dataframes = []

for filename in os.listdir(folder_name):
    if filename.endswith('.csv'):
        file_path = os.path.join(folder_name, filename)
        df = pd.read_csv(file_path)
        dataframes.append(df)

merged_df = pd.concat(dataframes, ignore_index=True)

merged_df['Label'].unique()
merged_df['Label'].value_counts()

merged_df = merged_df[merged_df['Label'] != 'Label']

# Convert all columns except 'Label' to numeric
for col in merged_df.columns:
    if col != 'Label':
        try:
            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')
        except:
            print(f"Could not convert column '{col}' to numeric.")

# Verify column types
print(merged_df.dtypes)

# Map Label to Attack type according to https://www.unb.ca/cic/datasets/ids-2018.html
# "Others" means not found in ntop supported protocols
# "Video" is included in "Streaming"
mapping = {
    'Benign':'Benign',
    'DDOS attack-HOIC':'DDoS+PortScan',
    'DDoS attacks-LOIC-HTTP':'DDoS+PortScan',
    'DoS attacks-Hulk':'DoS attack',
    'Bot':'Botnet attack',
    'FTP-BruteForce':'Bruteforce attack',
    'SSH-Bruteforce':'Bruteforce attack',
    'Infilteration':'Infiltration attack',
    'DoS attacks-SlowHTTPTest':'DoS attack',
    'DoS attacks-GoldenEye':'DoS attack',
    'DoS attacks-Slowloris':'DoS attack',
    'DDOS attack-LOIC-UDP':'DDoS+PortScan',
    'Brute Force -Web':'Web attack',
    'Brute Force -XSS':'Web attack',
    'SQL Injection':'Web attack',
  }
merged_df['AttackType'] = merged_df['Label'].map(mapping)
merged_df['AttackType'].value_counts()

merge_df.shape[0]

df = merge_df

"""# **Data Preprocessing**"""

# Encode Labels
label_mapping = {'Web attack': 1, 'Infiltration attack': 2, 'Botnet attack': 3, 'Bruteforce attack': 4, 'DoS attack': 5, 'DDoS+PortScan': 6, 'Benign': 0}
df['Attack_encoded'] = df['AttackType'].map(label_mapping)

# Drop non-numerical column
df.drop(['Timestamp','Flow ID','Src IP','Src Port','Dst IP'], axis=1, inplace=True)

# Separate features (x) and target variable (y)
x = df.drop(['AttackType','Attack_encoded','Label'], axis=1)
y = df['Attack_encoded']

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Replace inf and large values with NaN
x_train = x_train.replace([np.inf, -np.inf], np.nan)
# Impute NaN values (e.g., with the mean)
x_train = x_train.fillna(x_train.mean())

# Fitting XGB model for Feature Selection
xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)
xgb_model.fit(x_train, y_train)

# Get feature importances
feature_importances = xgb_model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': x_train.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df

# Select Top 50 Features
top_n = 50
selected_features = feature_importance_df.iloc[:top_n]['Feature'].tolist()

# Use only selected features for further training
x_train_selected = x_train[selected_features]
x_test_selected = x_test[selected_features]

# Replace inf and large values with NaN
x_train_selected = x_train_selected.replace([np.inf, -np.inf], np.nan)
x_test_selected = x_test_selected.replace([np.inf, -np.inf], np.nan)
# Impute NaN values (e.g., with the mean)
x_train_selected = x_train_selected.fillna(x_train.mean())
x_test_selected = x_test_selected.fillna(x_train.mean())

!pip install imblearn

from imblearn.over_sampling import SMOTE

# Handle Class Imbalance
smote = SMOTE()
x_train_resampled, y_train_resampled = smote.fit_resample(x_train_selected, y_train)

# Scaling data
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train_resampled)
x_test_scaled = scaler.transform(x_test_selected)

"""# **Model Training: Random Forest**"""

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 15, 20, 30, None],
    'max_features': ['sqrt', 'log2', 0.3, 0.5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 5],
    'bootstrap': [True, False]
}

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')

# Perform Randomized Search with Cross-Validation
rf_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

# Train model with best hyperparameters
rf_search.fit(x_train_scaled, y_train_resampled)

# Best parameters and score
print("Best Parameters:", rf_search.best_params_)
print("Best Score:", rf_search.best_score_)

# Train Best Model
best_rf = rf_search.best_estimator_
best_rf.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_rf, 'best_rf_model.pkl')

# Predictions
y_pred_RF = best_rf.predict(x_test_scaled)
y_prob_RF = best_rf.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_rf = accuracy_score(y_test, y_pred_RF)
print(f"Accuracy: {accuracy_rf:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_RF))
print("Classification Report:\n", classification_report(y_test, y_pred_RF))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_rf = cross_val_score(best_rf, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"RF Validation Score: {np.mean(cv_scores_rf):.4f} ± {np.std(cv_scores_rf):.4f}")

# ROC AUC score
roc_auc_rf = roc_auc_score(y_test, y_prob_RF, multi_class='ovr')
print(f"RF ROC AUC Score: {roc_auc_rf:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_rf = dict()
tpr_rf = dict()
roc_auc_per_class_rf = dict()

for i in range(n_classes):
    fpr_rf[i], tpr_rf[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_RF[:, i])
    roc_auc_per_class_rf[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_RF[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_rf[i], tpr_rf[i], label=f'Class {i} (AUC = {roc_auc_per_class_rf[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: Decision Tree**"""

# Hyperparameter Tuning for Decision Tree
param_grid = {
    'max_depth': [10, 15, 20, 30, None],  # Control over tree depth
    'max_features': ['sqrt', 'log2', 0.3, 0.5],  # Feature selection strategies
    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 3, 5],  # Minimum samples per leaf for regularization
    'criterion': ['gini', 'entropy'],  # Different impurity measures
    'splitter': ['best', 'random']  # Best split vs. random split selection
}

# Decision Tree Model
dt_model = DecisionTreeClassifier(random_state=42)

# RandomizedSearchCV for Decision Tree
dt_search = RandomizedSearchCV(
    estimator=dt_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

dt_search.fit(x_train_scaled, y_train_resampled)

# Best parameters and score
print("Best Parameters:", dt_search.best_params_)
print("Best Score:", dt_search.best_score_)

# Train Best Model
best_dt = dt_search.best_estimator_
best_dt.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_dt, 'best_dt_model.pkl')

# Predictions
y_pred_DT = best_dt.predict(x_test_scaled)
y_prob_DT = best_dt.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_dt = accuracy_score(y_test, y_pred_DT)
print(f"Accuracy: {accuracy_dt:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_DT))
print("Classification Report:\n", classification_report(y_test, y_pred_DT))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_dt = cross_val_score(best_dt, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"DT Validation Score: {np.mean(cv_scores_dt):.4f} ± {np.std(cv_scores_dt):.4f}")

# ROC AUC score
roc_auc_dt = roc_auc_score(y_test, y_prob_DT, multi_class='ovr')
print(f"DT ROC AUC Score: {roc_auc_dt:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_dt = dict()
tpr_dt = dict()
roc_auc_per_class_dt = dict()

for i in range(n_classes):
    fpr_dt[i], tpr_dt[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_DT[:, i])
    roc_auc_per_class_dt[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_DT[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_dt[i], tpr_dt[i], label=f'Class {i} (AUC = {roc_auc_per_class_dt[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: Support Vector Machine**"""

from sklearn.svm import SVC, LinearSVC
# Hyperparameter Tuning
param_grid = {
    'C': [0.1, 1, 10]
}

svm_model = LinearSVC(random_state=42, dual=False, max_iter=5000)

svm_search = RandomizedSearchCV(
    estimator=svm_model,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    scoring='accuracy',
    return_train_score=True
)

svm_search.fit(x_train_scaled, y_train_resampled)

# Best parameters and score
print("Best Parameters:", svm_search.best_params_)
print("Best Score:", svm_search.best_score_)

# Train Best Model
best_svm = svm_search.best_estimator_
best_svm.fit(x_train_scaled, y_train_resampled)

# Save trained model
joblib.dump(best_svm, 'best_svm_model.pkl')

# Predictions
y_pred_SVM = best_svm.predict(x_test_scaled)

from sklearn.calibration import CalibratedClassifierCV
svm_model_prob = CalibratedClassifierCV(best_svm)
svm_model_prob.fit(x_train_scaled, y_train_resampled)
y_prob_SVM = svm_model_prob.predict_proba(x_test_scaled)

# Evaluation Metrics
accuracy_svm = accuracy_score(y_test, y_pred_SVM)
print(f"Accuracy: {accuracy_svm:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_SVM))
print("Classification Report:\n", classification_report(y_test, y_pred_SVM))

# Model Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_svm = cross_val_score(best_svm, x_train_scaled, y_train_resampled, cv=kf, scoring='accuracy', n_jobs=-1)
print(f"SVM Validation Score: {np.mean(cv_scores_svm):.4f} ± {np.std(cv_scores_svm):.4f}")

# ROC AUC score
roc_auc_svm = roc_auc_score(y_test, y_prob_SVM, multi_class='ovr')
print(f"SVM ROC AUC Score: {roc_auc_svm:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_svm = dict()
tpr_svm = dict()
roc_auc_per_class_svm = dict()

for i in range(n_classes):
    fpr_svm[i], tpr_svm[i], _ = roc_curve(to_categorical(y_test)[:, i], y_prob_SVM[:, i])
    roc_auc_per_class_svm[i] = roc_auc_score(to_categorical(y_test)[:, i], y_prob_SVM[:, i])


plt.figure(figsize=(10,8))
for i in range(n_classes):
    plt.plot(fpr_svm[i], tpr_svm[i], label=f'Class {i} (AUC = {roc_auc_per_class_svm[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()

"""# **Model Training: Deep Neural Networks**"""

# Apply One-Hot Encoding Labels
y_train_cat = to_categorical(y_train_resampled, num_classes=7)
y_test_cat = to_categorical(y_test, num_classes=7)

# Hyperparameter Tuning
def build_dnn_model(hp):
    model = Sequential([
        Dense(hp.Int('units_1', min_value=32, max_value=512, step=32), input_dim=x_train_scaled.shape[1]),
        Activation(hp.Choice('activation_1', ['relu', 'elu', 'tanh'])),
        BatchNormalization(),
        Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)),

        Dense(hp.Int('units_2', min_value=32, max_value=512, step=32)),
        Activation(hp.Choice('activation_2', ['relu', 'elu', 'tanh'])),
        BatchNormalization(),
        Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)),

        Dense(7, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),
        loss='categorical_crossentropy',
        metrics=['accuracy']
  )

    return model

# Hyperparameter Tuning using Random Search
tuner = kt.RandomSearch(
    build_dnn_model,
    objective='val_accuracy',
    max_trials=50,
    executions_per_trial=2,
    directory='dnn_tuning',
    project_name='IDS_DNN',
    overwrite=True,
    max_consecutive_failed_trials=100
)

early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# Hyperparameter tuning in the reinforcement learning context
tuner.search(x_train_scaled, y_train_cat, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stopping])

# Train Best Model
best_dnnhps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best Units 1: {best_dnnhps.get('units_1')}")
print(f"Best Activation 1: {best_dnnhps.get('activation_1')}")
print(f"Best Dropout 1: {best_dnnhps.get('dropout_1')}")
print(f"Best Units 2: {best_dnnhps.get('units_2')}")
print(f"Best Activation 2: {best_dnnhps.get('activation_2')}")
print(f"Best Dropout 2: {best_dnnhps.get('dropout_2')}")
best_dnn = tuner.hypermodel.build(best_dnnhps)
best_dnn.fit(x_train_scaled, y_train_cat, epochs=100, batch_size=32, validation_split=0.2,callbacks=[early_stopping])

# Save trained model
best_dnn.save('best_dnn_model.h5')

# Predictions
y_prob_DNN = best_dnn.predict(x_test_scaled)
y_pred_DNN = np.argmax(y_prob_DNN, axis=1)

# Evaluation Metrics
accuracy_dnn = accuracy_score(y_test, y_pred_DNN)
print(f"Accuracy: {accuracy_dnn:.4f}")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_DNN))
print("Classification Report:\n", classification_report(y_test, y_pred_DNN))

# Model Validation
kf_dnn = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_dnn = []

for fold, (train_index, val_index) in enumerate(kf_dnn.split(x_train_scaled, y_train_resampled)):
    print(f"Training Fold {fold + 1}...")

    x_train_fold, x_val_fold = x_train_scaled[train_index], x_train_scaled[val_index]
    y_train_fold, y_val_fold = y_train_cat[train_index], y_train_cat[val_index]

    # Reset model before each fold to avoid contamination
    fold_model_dnn = tuner.hypermodel.build(best_dnnhps)

    fold_model_dnn.fit(x_train_fold, y_train_fold,epochs=100, batch_size=32, verbose=0,validation_data=(x_val_fold, y_val_fold),callbacks=[early_stopping])

    val_loss_dnn, val_acc_dnn = fold_model_dnn.evaluate(x_val_fold, y_val_fold, verbose=0)
    cv_scores_dnn.append(val_acc_dnn)

print(f"DNN Validation Score: {np.mean(cv_scores_dnn):.4f} ± {np.std(cv_scores_dnn):.4f}")

# ROC AUC score
roc_auc_dnn = roc_auc_score(y_test_cat, y_prob_DNN, multi_class='ovr')
print(f"DNN ROC AUC Score: {roc_auc_dnn:.4f}")

# Plot ROC Curve for each class
n_classes = len(np.unique(y_test))
fpr_dnn = dict()
tpr_dnn = dict()
roc_auc_per_class_dnn = dict()

for i in range(n_classes):
    fpr_dnn[i], tpr_dnn[i], _ = roc_curve(y_test_cat[:, i], y_prob_DNN[:, i])
    roc_auc_per_class_dnn[i] = roc_auc_score(y_test_cat[:, i], y_prob_DNN[:, i])

plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr_dnn[i], tpr_dnn[i], label=f'Class {i} (AUC = {roc_auc_per_class_dnn[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for each class')
plt.legend(loc="lower right")
plt.show()